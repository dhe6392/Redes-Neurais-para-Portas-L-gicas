# Redes Neurais para Portas L√≥gicas

Este projeto implementa uma rede neural simples, criada do zero em Python com NumPy, para aprender e visualizar os principais operadores l√≥gicos (AND, OR, NOT, XOR).

## üìÅ Estrutura dos Arquivos

- `Perceptron.py`: Implementa um perceptron de camada √∫nica capaz de aprender as portas **AND**, **OR** e **NOT**, que s√£o linearmente separ√°veis. Tamb√©m inclui uma tentativa de aplicar esse modelo ao **XOR**, que falha, pois XOR **n√£o** √© linearmente separ√°vel.
- `XOR.py`: Implementa uma rede neural com **camada oculta** e **backpropagation**, permitindo que o modelo aprenda a resolver corretamente a opera√ß√£o **XOR**. Tamb√©m foi testado e aprovado para as outras portas.
- `AND.png`, `OR.png`, `NOT.png`: Gr√°ficos de entrada/sa√≠da com as respectivas **retas de separa√ß√£o linear** aprendidas para cada porta l√≥gica.
- `XOR.png`: Gr√°fico dos dados da fun√ß√£o XOR, evidenciando a **n√£o-linearidade** que impede sua separa√ß√£o por uma √∫nica reta.

## üöÄ Execu√ß√£o

1. **Executar o `Perceptron.py`**:
   - Treina um neur√¥nio simples para aprender AND, OR e NOT.
   - Mostra os par√¢metros aprendidos e exibe os gr√°ficos correspondentes com a separa√ß√£o dos dados.

2. **Executar o `XOR.py`**:
   - Constr√≥i uma rede com 2 neur√¥nios na camada oculta e 1 de sa√≠da.
   - Usa a fun√ß√£o de ativa√ß√£o **sigmoid** e aplica **backpropagation** para treinar a rede.
   - Resolve com sucesso a opera√ß√£o XOR e imprime as previs√µes.

## üß† Conceitos Envolvidos

- **Perceptron**: Algoritmo de aprendizado para dados linearmente separ√°veis.
- **Fun√ß√£o degrau**: Usada em `Perceptron.py` para simular a ativa√ß√£o bin√°ria (0 ou 1).
- **Feedforward**: Fluxo em que cada neur√¥nio transforma entradas em sa√≠das, alimentando os neur√¥nios seguintes.
- **Sigmoid + Backpropagation**: Usada em `XOR.py` para permitir ajustes cont√≠nuos dos pesos e possibilitar o aprendizado n√£o linear.
- **Linearidade**: Apenas AND, OR e NOT s√£o linearmente separ√°veis com um √∫nico neur√¥nio.
- **Camadas ocultas**: Necess√°rias para resolver o XOR, pois ele exige **composi√ß√£o de fun√ß√µes n√£o-lineares**.

## üìä Visualiza√ß√µes

Cada gr√°fico mostra os pontos de entrada da opera√ß√£o l√≥gica com suas respectivas sa√≠das esperadas (cores e r√≥tulos), al√©m da reta de separa√ß√£o (quando poss√≠vel).

Legenda:
- Azul = sa√≠da esperada 0
- Vermelho = sa√≠da esperada 1
- Linha tracejada = fronteira de decis√£o aprendida

## ‚úÖ Resultados Esperados

| Operador | Acerto com 1 neur√¥nio      | Gr√°fico com linha separadora? |
|----------|----------------------------|-------------------------------|
| AND      | ‚úÖ                        | ‚úÖ                            |
| OR       | ‚úÖ                        | ‚úÖ                            |
| NOT      | ‚úÖ                        | ‚úÖ                            |
| XOR      | ‚ùå (com 1 neur√¥nio)       | ‚ùå (n√£o-linear)               |
| XOR      | ‚úÖ (com 3 neur√¥nios)      | üîÅ Requer rede com backprop   |

## üìå Observa√ß√µes

- O `Perceptron.py` mostra que o XOR n√£o pode ser resolvido com um √∫nico neur√¥nio.
- O `XOR.py` resolve o problema corretamente, simulando uma **rede neural multicamada** e mostrando como o aprendizado profundo permite capturar padr√µes mais complexos.
- O aprendizado em XOR.py pode n√£o convergir dependendo da sorte na inicializa√ß√£o aleat√≥ria dos pesos, que pode gerar casos desfavor√°veis e lentificar o treinamento.

## üìö Requisitos

- Python 3.x
- `numpy`
- `matplotlib`

