# Redes Neurais para Portas L√≥gicas

Este projeto implementa uma rede neural simples, criada do zero em Python com NumPy, para aprender e visualizar os principais operadores l√≥gicos (AND, OR, NOT, XOR).

## üìÅ Estrutura dos Arquivos

- `Perceptron.py`: Implementa um perceptron de camada √∫nica capaz de aprender as portas **AND**, **OR** e **NOT**, que s√£o linearmente separ√°veis. Tamb√©m inclui uma tentativa de aplicar esse modelo ao **XOR**, que falha, pois XOR **n√£o** √© linearmente separ√°vel.
- `XOR.py`: Implementa uma rede neural com **camada oculta** e **backpropagation**, permitindo que o modelo aprenda a resolver corretamente a opera√ß√£o **XOR**. Tamb√©m foi testado e aprovado para as outras portas.
- `scikit.py`: Implementa uma rede neural utilizando a biblioteca `scikit-learn` (`MLPClassifier`) para resolver as mesmas portas l√≥gicas. Serve como base de compara√ß√£o com a rede feita √† m√£o.
- `AND.png`, `OR.png`, `NOT.png`: Gr√°ficos de entrada/sa√≠da com as respectivas **retas de separa√ß√£o linear** aprendidas para cada porta l√≥gica.
- `XOR.png`: Gr√°fico dos dados da fun√ß√£o XOR, evidenciando a **n√£o linearidade** que impede sua separa√ß√£o por uma √∫nica reta.

## üöÄ Execu√ß√£o

1. **Executar o `Perceptron.py`**:
   - Treina um neur√¥nio simples para aprender AND, OR e NOT.
   - Mostra os par√¢metros aprendidos e exibe os gr√°ficos correspondentes com a separa√ß√£o dos dados.

2. **Executar o `XOR.py`**:
   - Constr√≥i uma rede com 2 neur√¥nios na camada oculta e 1 de sa√≠da.
   - Usa a fun√ß√£o de ativa√ß√£o **sigmoid** e aplica **backpropagation** para treinar a rede.
   - Resolve com sucesso a opera√ß√£o XOR e imprime as previs√µes.

3. **Executar o `scikit.py`**:
   - Utiliza a rede neural `MLPClassifier` da biblioteca scikit-learn.
   - Treina uma rede multicamada de forma otimizada para resolver AND, OR e XOR com alto desempenho.

## üß† Conceitos Envolvidos

- **Perceptron**: Algoritmo de aprendizado para dados linearmente separ√°veis.
- **Fun√ß√£o degrau**: Usada em `Perceptron.py` para simular a ativa√ß√£o bin√°ria (0 ou 1).
- **Feedforward**: Fluxo em que cada neur√¥nio transforma entradas em sa√≠das, alimentando os neur√¥nios seguintes.
- **Sigmoid + Backpropagation**: Usada em `XOR.py` para permitir ajustes cont√≠nuos dos pesos e possibilitar o aprendizado n√£o linear.
- **Linearidade**: Apenas AND, OR e NOT s√£o linearmente separ√°veis com um √∫nico neur√¥nio.
- **Camadas ocultas**: Necess√°rias para resolver o XOR, pois ele exige **composi√ß√£o de fun√ß√µes n√£o lineares**.
- **MLPClassifier (scikit-learn)**: Algoritmo robusto de rede neural multicamada com t√©cnicas de otimiza√ß√£o avan√ßadas.

## üìä Visualiza√ß√µes

Cada gr√°fico mostra os pontos de entrada da opera√ß√£o l√≥gica com suas respectivas sa√≠das esperadas (cores e r√≥tulos), al√©m da reta de separa√ß√£o (quando poss√≠vel).

**Legenda:**
- Azul = sa√≠da esperada 0  
- Vermelho = sa√≠da esperada 1  
- Linha tracejada = fronteira de decis√£o aprendida

## ‚úÖ Resultados Esperados

| Operador | Acerto com 1 neur√¥nio      | Gr√°fico com linha separadora? |
|----------|----------------------------|-------------------------------|
| AND      | ‚úÖ                         | ‚úÖ                            |
| OR       | ‚úÖ                         | ‚úÖ                            |
| NOT      | ‚úÖ                         | ‚úÖ                            |
| XOR      | ‚ùå (com 1 neur√¥nio)        | ‚ùå (n√£o linear)               |
| XOR      | ‚úÖ (com 3 neur√¥nios)       | üîÅ Requer rede com backprop   |

## üìå Observa√ß√µes

- O `Perceptron.py` mostra que o XOR n√£o pode ser resolvido com um √∫nico neur√¥nio.
- O `XOR.py` resolve o problema corretamente, simulando uma **rede neural multicamada** e mostrando como o aprendizado profundo permite capturar padr√µes mais complexos.
- O aprendizado em `XOR.py` pode n√£o convergir, dependendo da sorte na inicializa√ß√£o aleat√≥ria dos pesos, que pode gerar casos desfavor√°veis e tornar o treinamento lento.
- O `scikit.py` usa uma rede neural pronta (`MLPClassifier`), que **resolve todos os operadores com extrema rapidez e estabilidade**. Isso ocorre porque:
  - Os pesos s√£o inicializados de forma otimizada (ex.: Xavier)
  - O algoritmo de treinamento √© mais eficiente (como Adam)
  - A biblioteca faz ajustes autom√°ticos na taxa de aprendizado
  - A implementa√ß√£o √© altamente otimizada em Cython

Essa compara√ß√£o destaca como solu√ß√µes feitas √† m√£o s√£o valiosas para aprendizado, mas ferramentas especializadas como o `scikit-learn` s√£o muito mais eficazes na pr√°tica.

